{
# First, the "hidden layer" (Z1, Z2, ...Z4).  Z is an N x 4 matrix:
Alpha0 <- get_Alpha0_vector(eta)
Alpha <- get_Alpha_matrix(eta)
# the t(t(...)) is a trick to add the Alpha0 to the rows
Z_no_sigma <- t(t(X %*% Alpha) + Alpha0)
Z <- sigma(Z_no_sigma)
# Next, the "output layer" (T1, T2) (call output "TT" to not confuse with "TRUE" in R):
# TT is be an N x 2 matrix
beta0 <- get_beta0_vector(eta)
beta <- get_beta_matrix(eta)
TT_no_sigma <- t(t(Z %*% beta) + beta0)
TT <- sigma(TT_no_sigma)
# Last, the probability of Y1, Y2 given the x input:
denom <- rowSums(exp(TT))
Y <- exp(TT)/denom
return (Y)
}
#######################
#######Question 2c
#######################
#Define an arbitrary eta:
set.seed(2221);eta <- runif(22, -1,1)
X = nn[,1:2]
#We previously defined logL. But now we must make it negative:
# Likelihood functions
logL_neg <- function(eta, X, y, rho = 0)
{
Y <- NN(eta, X)
logvals <- ifelse(y == 1, log(Y[,2]), log(Y[,1]))
logL <- sum(logvals) - rho*sum(eta*eta)
return (-logL)
}
#Test:
logL(eta, X, nn[,3], rho=0)
logL_neg(eta, as.matrix(X), as.matrix(nn[,3]), rho=0) #This is the opposite of logL(), which is good.
grad_logL_neg <- function(eta, X, y, rho = 0)
{
h = 10^-7
g <- rep(0, 22)
base_value <- logL_neg(eta, X, y, rho)
for (i in 1:22) {
eta_new <- eta
eta_new[i] <- eta_new[i] + h
g[i] <- logL_neg(eta_new, X, y, rho) - base_value
g[i] <- g[i]/h
}
return (g)
}
grad_logL_neg(eta, as.matrix(X), as.matrix(nn[,3]), rho=0)
grad_logL(eta, X, nn[,3], rho = 0)  #This is the opposite of logL(), which is good.
Hf_d = function(eta, d, X, y, rho = 0, eps = 10^-2){
num <- grad_logL_neg(eta + eps*d, X, y, rho = rho) - grad_logL_neg(eta, X, y, rho = rho)
Hf_d = num/eps
return(Hf_d)
}
set.seed(2221)
eta <- runif(22, -1,1)
d <- rep(1, 22)
Hf_d(eta, d, as.matrix(X) , as.matrix(nn[,3]))
#This version accepts a vector instead of a matrix:
options(scipen=999)
set.seed(2221)
eta <- runif(22, -.1,.1)
b = -grad_logL_neg(eta, as.matrix(X) , as.matrix(nn[,3]))
CGM_v <- function(b, lambda = 100){
digits = 10
x = grad_logL_neg(eta, X = X, y = nn[,3])/lambda  #This was a good starting point.
r = b - (Hf_d(eta, d=x, X = X, y = nn[,3]) + lambda*x)
d = r
iter = 0
for(m in 1:30){
iter = iter + 1
Ad = Hf_d(eta, d, X=X, y = nn[,3]) + lambda*d
a = as.numeric((t(r)%*%r)/t(d)%*%Ad)
x_new = x + a*d
r_new = r - a*(Ad)
B = as.numeric((t(r_new)%*%r_new)/t(r)%*%r)
d_new = r_new + B*d
d = d_new
r = r_new
x = x_new
}
#print(paste0("Iter = ",iter))
return(x)
}
grad_logL_neg(eta, as.matrix(X), as.matrix(nn[,3]), rho=0)
CGM_v(b = -grad_logL_neg(as.matrix(eta), as.matrix(nn[,-3]) , as.matrix(nn[,3] ), lambda = 100))  #Iterations are 3.
digits = 10
x = grad_logL_neg(eta, X = X, y = nn[,3])/lambda  #This was a good starting point.
r = b - (Hf_d(eta, d=x, X = X, y = nn[,3]) + lambda*x)
d = r
grad_logL_neg
x
x = grad_logL_neg(eta, X = X, y = nn[,3])/lambda  #This was a good starting point.
r = b - (Hf_d(eta, d=x, as.matrix(X), as.matrix(nn[,3]) ) + lambda*x)
x = grad_logL_neg(eta, as.matrix(X), as.matrix(nn[,3]), rho=0)/lambda  #This was a good starting point.
CGM_v <- function(b, lambda = 100){
digits = 10
x = grad_logL_neg(eta, as.matrix(X), as.matrix(nn[,3]), rho=0)/lambda  #This was a good starting point.
r = b - (Hf_d(eta, d=x, as.matrix(X), as.matrix(nn[,3]) ) + lambda*x)
d = r
iter = 0
for(m in 1:30){
iter = iter + 1
Ad = Hf_d(eta, d, X=X, y = nn[,3]) + lambda*d
a = as.numeric((t(r)%*%r)/t(d)%*%Ad)
x_new = x + a*d
r_new = r - a*(Ad)
B = as.numeric((t(r_new)%*%r_new)/t(r)%*%r)
d_new = r_new + B*d
d = d_new
r = r_new
x = x_new
}
#print(paste0("Iter = ",iter))
return(x)
}
grad_logL_neg(eta, as.matrix(X), as.matrix(nn[,3]), rho=0)
CGM_v(b = -grad_logL_neg(as.matrix(eta), as.matrix(nn[,-3]) , as.matrix(nn[,3] ), lambda = 100))  #Iterations are 3.
lambda = 100
r = b - (Hf_d(eta, d=x, as.matrix(X), as.matrix(nn[,3]) ) + lambda*x)
x = grad_logL_neg(eta, as.matrix(X), as.matrix(nn[,3]), rho=0)/lambda  #This was a good starting point.
r = b - (Hf_d(eta, d=x, as.matrix(X), as.matrix(nn[,3]) ) + lambda*x)
d = r
#Selina Carter
# MATH-504: HW #12
#May 9, 2018
rm(list = ls())
setwd("G:\\math\\504")
options(scipen=999)
Norm <- function(w){  sqrt(sum(w^2))}
require(ggplot2)
nn <- read.table("nn.txt", header = TRUE)
nn$y <- as.factor(nn$y)
#####################################
##Functions to return elements of eta
#####################################
# return Alpha_0^i
get_Alpha0 <- function(eta, i)
{   return (eta[3*i-2])   }
# collect all Alpha0 as a vector
get_Alpha0_vector <- function(eta)
{
vec <- rep(0, 4)
for (i in 1:4)
vec[i] <- get_Alpha0(eta, i)
return (vec)
}
# return Alpha^i
get_Alpha <- function(eta, i)
{  return (c(eta[3*i-1], eta[3*i]))  }
# return beta_0^i
get_beta0 <- function(eta, i)
{  return (eta[5*i-4 + 12])  }
# collect all beta0 as a vector
get_beta0_vector <- function(eta)
{
vec <- rep(0, 2)
for (i in 1:2)
vec[i] <- get_beta0(eta, i)
return (vec)
}
# return beta^i
get_beta <- function(eta, i)
{
return (c(eta[5*i-3 + 12],
eta[5*i-2 + 12],
eta[5*i-1 + 12],
eta[5*i + 12]))
}
# for fast computation in NN, form a matrix with the Alphas as columns
get_Alpha_matrix <- function(eta)
{
# a bit of a hack, but...
all_Alphas <- c(get_Alpha(eta, 1), get_Alpha(eta, 2),
get_Alpha(eta, 3), get_Alpha(eta, 4))
m <- matrix(all_Alphas, ncol=4)
return(m)
}
get_Alpha_matrix(eta)
get_beta_matrix <- function(eta)
{
# a bit of a hack, but...
all_betas <- c(get_beta(eta, 1), get_beta(eta, 2))
m <- matrix(all_betas, ncol=2)
return(m)
}
get_beta_matrix(eta)
#The sigma function is used throughout the Neural Net,:
sigma <- function(z)
{  return (1/(1 + exp(-z))) }
#The Neural Net function:
NN <- function(eta, X)
{
# First, the "hidden layer" (Z1, Z2, ...Z4).  Z is an N x 4 matrix:
Alpha0 <- get_Alpha0_vector(eta)
Alpha <- get_Alpha_matrix(eta)
# the t(t(...)) is a trick to add the Alpha0 to the rows
Z_no_sigma <- t(t(X %*% Alpha) + Alpha0)
Z <- sigma(Z_no_sigma)
# Next, the "output layer" (T1, T2) (call output "TT" to not confuse with "TRUE" in R):
# TT is be an N x 2 matrix
beta0 <- get_beta0_vector(eta)
beta <- get_beta_matrix(eta)
TT_no_sigma <- t(t(Z %*% beta) + beta0)
TT <- sigma(TT_no_sigma)
# Last, the probability of Y1, Y2 given the x input:
denom <- rowSums(exp(TT))
Y <- exp(TT)/denom
return (Y)
}
#######################
#######Question 2c
#######################
#Define an arbitrary eta:
set.seed(2221);eta <- runif(22, -1,1)
X = nn[,1:2]
#We previously defined logL. But now we must make it negative:
# Likelihood functions
logL_neg <- function(eta, X, y, rho = 0)
{
Y <- NN(eta, X)
logvals <- ifelse(y == 1, log(Y[,2]), log(Y[,1]))
logL <- sum(logvals) - rho*sum(eta*eta)
return (-logL)
}
#Test:
logL(eta, X, nn[,3], rho=0)
logL_neg(eta, as.matrix(X), as.matrix(nn[,3]), rho=0) #This is the opposite of logL(), which is good.
#Gradient function:
grad_logL_neg <- function(eta, X, y, rho = 0)
{
h = 10^-7
g <- rep(0, 22)
base_value <- logL_neg(eta, X, y, rho)
for (i in 1:22) {
eta_new <- eta
eta_new[i] <- eta_new[i] + h
g[i] <- logL_neg(eta_new, X, y, rho) - base_value
g[i] <- g[i]/h
}
return (g)
}
grad_logL_neg(eta, as.matrix(X), as.matrix(nn[,3]), rho=0)
grad_logL(eta, X, nn[,3], rho = 0)  #This is the opposite of logL(), which is good.
##Compute the finite approximation for Hf(eta)d, using different epsilon values:
#My numbers were closest to a colleague's Hf_d by using epsilon = 10^-2
Hf_d = function(eta, d, X, y, rho = 0, eps = 10^-2){
num <- grad_logL_neg(eta + eps*d, X, y, rho = rho) - grad_logL_neg(eta, X, y, rho = rho)
Hf_d = num/eps
return(Hf_d)
}
set.seed(2221)
eta <- runif(22, -1,1)
d <- rep(1, 22)
Hf_d(eta, d, as.matrix(X) , as.matrix(nn[,3]))
#######################
#######Question 2d
#######################
#This version accepts a vector instead of a matrix:
options(scipen=999)
set.seed(2221)
eta <- runif(22, -.1,.1)
b = -grad_logL_neg(eta, as.matrix(X) , as.matrix(nn[,3]))
CGM_v <- function(b, lambda = 100){
digits = 10
x = grad_logL_neg(eta, as.matrix(X), as.matrix(nn[,3]), rho=0)/lambda  #This was a good starting point.
r = b - (Hf_d(eta, x, as.matrix(X), as.matrix(nn[,3]) ) + lambda*x)
d = r
iter = 0
for(m in 1:30){
iter = iter + 1
Ad = Hf_d(eta, d, X=X, y = nn[,3]) + lambda*d
a = as.numeric((t(r)%*%r)/t(d)%*%Ad)
x_new = x + a*d
r_new = r - a*(Ad)
B = as.numeric((t(r_new)%*%r_new)/t(r)%*%r)
d_new = r_new + B*d
d = d_new
r = r_new
x = x_new
}
#print(paste0("Iter = ",iter))
return(x)
}
grad_logL_neg(eta, as.matrix(X), as.matrix(nn[,3]), rho=0)
CGM_v(b = -grad_logL_neg(as.matrix(eta), as.matrix(nn[,-3]) , as.matrix(nn[,3] ), lambda = 100))  #Iterations are 3.
CGM_v(b = -grad_logL_neg(as.matrix(eta), as.matrix(nn[,-3]) , as.matrix(nn[,3] ),   100))  #Iterations are 3.
lambda
CGM_v <- function(b, lambda = 100){
digits = 10
x = grad_logL_neg(eta, as.matrix(X), as.matrix(nn[,3]), rho=0)/lambda  #This was a good starting point.
r = b - (Hf_d(eta, x, as.matrix(X), as.matrix(nn[,3]) ) + lambda*x)
d = r
iter = 0
for(m in 1:30){
iter = iter + 1
Ad = Hf_d(eta, d, as.matrix(X), as.matrix(nn[,3])) + lambda*d
a = as.numeric((t(r)%*%r)/t(d)%*%Ad)
x_new = x + a*d
r_new = r - a*(Ad)
B = as.numeric((t(r_new)%*%r_new)/t(r)%*%r)
d_new = r_new + B*d
d = d_new
r = r_new
x = x_new
}
#print(paste0("Iter = ",iter))
return(x)
}
grad_logL_neg(eta, as.matrix(X), as.matrix(nn[,3]), rho=0)
CGM_v(b = -grad_logL_neg(as.matrix(eta), as.matrix(nn[,-3]) , as.matrix(nn[,3] ),   100))  #Iterations are 3.
#Selina Carter
# MATH-504: HW #12
#May 9, 2018
rm(list = ls())
setwd("G:\\math\\504")
options(scipen=999)
Norm <- function(w){  sqrt(sum(w^2))}
require(ggplot2)
nn <- read.table("nn.txt", header = TRUE)
nn$y <- as.factor(nn$y)
#####################################
##Functions to return elements of eta
#####################################
# return Alpha_0^i
get_Alpha0 <- function(eta, i)
{   return (eta[3*i-2])   }
# collect all Alpha0 as a vector
get_Alpha0_vector <- function(eta)
{
vec <- rep(0, 4)
for (i in 1:4)
vec[i] <- get_Alpha0(eta, i)
return (vec)
}
# return Alpha^i
get_Alpha <- function(eta, i)
{  return (c(eta[3*i-1], eta[3*i]))  }
# return beta_0^i
get_beta0 <- function(eta, i)
{  return (eta[5*i-4 + 12])  }
# collect all beta0 as a vector
get_beta0_vector <- function(eta)
{
vec <- rep(0, 2)
for (i in 1:2)
vec[i] <- get_beta0(eta, i)
return (vec)
}
# return beta^i
get_beta <- function(eta, i)
{
return (c(eta[5*i-3 + 12],
eta[5*i-2 + 12],
eta[5*i-1 + 12],
eta[5*i + 12]))
}
# for fast computation in NN, form a matrix with the Alphas as columns
get_Alpha_matrix <- function(eta)
{
# a bit of a hack, but...
all_Alphas <- c(get_Alpha(eta, 1), get_Alpha(eta, 2),
get_Alpha(eta, 3), get_Alpha(eta, 4))
m <- matrix(all_Alphas, ncol=4)
return(m)
}
get_Alpha_matrix(eta)
get_beta_matrix <- function(eta)
{
# a bit of a hack, but...
all_betas <- c(get_beta(eta, 1), get_beta(eta, 2))
m <- matrix(all_betas, ncol=2)
return(m)
}
get_beta_matrix(eta)
#The sigma function is used throughout the Neural Net,:
sigma <- function(z)
{  return (1/(1 + exp(-z))) }
#The Neural Net function:
NN <- function(eta, X)
{
# First, the "hidden layer" (Z1, Z2, ...Z4).  Z is an N x 4 matrix:
Alpha0 <- get_Alpha0_vector(eta)
Alpha <- get_Alpha_matrix(eta)
# the t(t(...)) is a trick to add the Alpha0 to the rows
Z_no_sigma <- t(t(X %*% Alpha) + Alpha0)
Z <- sigma(Z_no_sigma)
# Next, the "output layer" (T1, T2) (call output "TT" to not confuse with "TRUE" in R):
# TT is be an N x 2 matrix
beta0 <- get_beta0_vector(eta)
beta <- get_beta_matrix(eta)
TT_no_sigma <- t(t(Z %*% beta) + beta0)
TT <- sigma(TT_no_sigma)
# Last, the probability of Y1, Y2 given the x input:
denom <- rowSums(exp(TT))
Y <- exp(TT)/denom
return (Y)
}
#######################
#######Question 2c
#######################
#Define an arbitrary eta:
set.seed(2221);eta <- runif(22, -1,1)
X = nn[,1:2]
#We previously defined logL. But now we must make it negative:
# Likelihood functions
logL_neg <- function(eta, X, y, rho = 0)
{
Y <- NN(eta, X)
logvals <- ifelse(y == 1, log(Y[,2]), log(Y[,1]))
logL <- sum(logvals) - rho*sum(eta*eta)
return (-logL)
}
#Test:
logL(eta, X, nn[,3], rho=0)
logL_neg(eta, as.matrix(X), as.matrix(nn[,3]), rho=0) #This is the opposite of logL(), which is good.
#Gradient function:
grad_logL_neg <- function(eta, X, y, rho = 0)
{
h = 10^-7
g <- rep(0, 22)
base_value <- logL_neg(eta, X, y, rho)
for (i in 1:22) {
eta_new <- eta
eta_new[i] <- eta_new[i] + h
g[i] <- logL_neg(eta_new, X, y, rho) - base_value
g[i] <- g[i]/h
}
return (g)
}
grad_logL_neg(eta, as.matrix(X), as.matrix(nn[,3]), rho=0)
grad_logL(eta, X, nn[,3], rho = 0)  #This is the opposite of logL(), which is good.
##Compute the finite approximation for Hf(eta)d, using different epsilon values:
#My numbers were closest to a colleague's Hf_d by using epsilon = 10^-2
Hf_d = function(eta, d, X, y, rho = 0, eps = 10^-2){
num <- grad_logL_neg(eta + eps*d, X, y, rho = rho) - grad_logL_neg(eta, X, y, rho = rho)
Hf_d = num/eps
return(Hf_d)
}
set.seed(2221)
eta <- runif(22, -1,1)
d <- rep(1, 22)
Hf_d(eta, d, as.matrix(X) , as.matrix(nn[,3]))
#######################
#######Question 2d
#######################
#This version accepts a vector instead of a matrix:
options(scipen=999)
set.seed(2221)
eta <- runif(22, -.1,.1)
b = -grad_logL_neg(eta, as.matrix(X) , as.matrix(nn[,3]))
CGM_v <- function(b, lambda = 100){
digits = 10
x = grad_logL_neg(eta, as.matrix(X), as.matrix(nn[,3]), rho=0)/lambda  #This was a good starting point.
r = b - (Hf_d(eta, x, as.matrix(X), as.matrix(nn[,3]) ) + lambda*x)
d = r
iter = 0
for(m in 1:30){
iter = iter + 1
Ad = Hf_d(eta, d, as.matrix(X), as.matrix(nn[,3])) + lambda*d
a = as.numeric((t(r)%*%r)/t(d)%*%Ad)
x_new = x + a*d
r_new = r - a*(Ad)
B = as.numeric((t(r_new)%*%r_new)/t(r)%*%r)
d_new = r_new + B*d
d = d_new
r = r_new
x = x_new
}
#print(paste0("Iter = ",iter))
return(x)
}
grad_logL_neg(eta, as.matrix(X), as.matrix(nn[,3]), rho=0)
CGM_v(b = -grad_logL_neg(as.matrix(eta), as.matrix(nn[,-3]) , as.matrix(nn[,3] ),   100))  #Iterations are 3.
set.seed(2221)
eta_start = runif(22, -.1, .1)
eta_try1 <- train_NN_Hf(eta_start, iterations =100)
train_NN_Hf
train_NN_Hf <- function(start_eta, iterations,
rho = 0,
file="nn.txt")
{
time_it <- proc.time()
data <- read.table(file, header=T)
X <- as.matrix(data[,1:2])
y <- data$y
eta <- start_eta
for (q in 1:iterations) {
##Define p
b = -grad_logL_neg(eta, X = X, y = y)
p = CGM_v(b, lambda = 100)
## Define new eta
eta_new = eta + p
eta = eta_new
print(paste0("iter = ", q))
if(q %% 10 == 0){
print(paste0("logL =", logL_neg(eta, X = X, y = y)))
}
}
# show time:
print(proc.time() - time_it)
return (eta)
}
set.seed(2221)
eta_start = runif(22, -.1, .1)
eta_try1 <- train_NN_Hf(eta_start, iterations =100)
eta
logL_neg(eta)
logL_neg
logL_neg(eta,as.matrix(X),as.matrix(nn[,3]),rho=0)
