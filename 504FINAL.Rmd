---
title: ''
output: pdf_document
geometry: margin=.9in
header-includes:
   - \usepackage{amssymb}
   - \usepackage{bm}
   - \usepackage{cancel}
---


```{r}
rm(list = ls())
 setwd("G:\\math\\504")
options(scipen=999)
 #require("ggplot2")
bones<-read.table("BoneMassDataF.txt",header=T)
bones<-bones[which(bones[,3] == "female"),]

Fapprox<-function(m,n=10000,x){

  MATT<-matrix(0:(m-1),m,1)

  Gx<-function(v,x){ if(v != 0) {
  	return( cos( (2*pi*v*(x-9.4 ) ) /(25.55-9.4 )) )
  	} else  {return(rep(1,length(x) ))} }

  a=min(x);b=max(x)
  h=(b-a)/n;i=0:(n-1)
  W<-apply(MATT,1,function(w) Gx(w,a+(i+1)*h) )  

  Mat<-matrix(NA,m,m)

  for( i in 1:m){
    Mat[,i] <-	 colSums( (  W[  ,i ]*W[  ,1:m]  ) ) * h } 

return(Mat)
}

Fapprox(6,n=10000, bones$age)

```

```{r}
Gapprox<-function(n,j,k,x){
	
	coeff<-(1/sqrt( diag(Fapprox(6,n=10000, bones$age)) )) 
	Fx<-function(l,x){ if(l != 0) {
		return( coeff[l+1]* cos( (2*pi*l*(x-9.4 ) ) /(25.55-9.4 )) )
		} else  {return( coeff[l+1]* rep(1,length(x) ))} }

	a=min(x);b=max(x)
	
	h=(b-a)/n;i=0:(n-1)
	return( 	sum( Fx(j,a+(i+1)*h)*Fx(k,a+(i+1)*h)*h )	)
}

matt<-matrix(NA,6,6)
for( J in 0:5){for( K in 0:5){ matt[J+1,K+1]<-Gapprox(10000,J,K,bones[,2]) }}
matt;diag(matt )

```


\ 

In this step, I am writing a function for any general $b_j$. If \text{w} is 1,...,5, I obtain the normalizing coefficients, apply them to the $b_j = 1,...,5$, and return the specified $b_j$. If 0 is given I return 1's. 

\ 


```{r}
Fx<-function(w,x){   
	#need this matrix for the normalizing Coefficients
	ncoef<-(1/sqrt( diag(Fapprox(6,n=10000, bones$age)) )) 
	if(w == 0 ) {return(   rep(1,length(x) ))  } else {
	return( ncoef[w+1]* cos( (2*pi*w*(x-9.4 ) ) /(25.55-9.4 )) )
		} }
```


\ 

In this step I form the model matrix. By using our function \text{Fx} above. Then I solve for $\alpha$.

\ 




```{r}
model_matrix <- function(x,m) { 
	nx <- length(x)
	A<-matrix(NA,nx,m+1)
	for( i in 0:m){
		A[,(i+1)] <- Fx(i,x)  }
	colnames(A )<-NULL
	return(A)	}


B<- model_matrix(bones$age,5)


Alpha <- solve(t(B) %*% B , t(B) %*% as.matrix(bones[,4])) 
```



```{r}
x_grid <- seq(min(bones$age), max(bones$age), .01)
B_grid <- model_matrix(x_grid,5)
y_grid <- B_grid %*% Alpha
 
par(mar=c(4.1,4.1,2,1))
plot(bones$age, bones[,4], ylim=c(min(bones[,4])-.02,max(bones[,4]) + .02) )
lines(x_grid, y_grid, col="red", lwd=2)   
```

\ 



We have some data, $x^{(i)} \in \mathbb{R}$ and $y_i \in \mathbb{R}$, and we wish to approximate $y$ by a function $f(x)$, when $f(x) \in \mathcal{F}$ and $f(x): \mathbb{R}^n \rightarrow \mathbb{R}$.

\ 

Let $\mathcal{F}$ be:

\ 

\begin{equation*} 
\mathcal{F} = \{ f(x): f(x): \mathbb{R}^n \rightarrow \mathbb{R}, f(x) = 5^{th} \text{ harmonic} \} . 
\end{equation*}

\ 

We use least squares to find the $f(x) \in \mathcal{F}$

\ 

\begin{align}
     \min\limits_{f(x) \in \mathcal{F} } \sum\limits_{i=1}^N \left| y_i - f\left(x^{(i)} \right) \right|
\end{align} 

 
 \ 
 
 It is key to note that $\mathcal{F}$ is a linear function / vector space. It is true that for any $g(x) \in \mathcal{F}$ and $h(x) \in \mathcal{F}$, $c_1 g(x) + c_2 h(x) \in \mathcal{F}$, where $c_1,c_2 \in \mathbb{R}$
 
 \ 
 
 
 Our function $f(x)$ can be written as:
 
 \ 
 
\begin{align*} f(x) &= \sum_{j=0}^n \alpha_j b_j(x) = \alpha_0 (1) + \alpha_1 \cos\left(
    \cfrac{2\pi(1)\left(x-x_{max}\right)}{\left(x_{max}-x_{min}\right)} \right) + ...  + \alpha_5 \cos\left(
    \cfrac{2\pi(5)\left(x-x_{max}\right)}{\left(x_{max}-x_{min}\right)} \right) 
    \end{align*} 
 
 \ 
 
 so we can rewrite (1) as:
 
\ 

\hspace{-1 cm}
\begin{align}
     \min\limits_{f(x) \in \mathcal{F} } \sum\limits_{i=1}^N \left| y_i - f\left(x^{(i)} \right) \right| =
     \min\limits_{\alpha \in \mathbb{R}^6 } \sum\limits_{i=1}^N \left|\alpha_0 + \alpha_1 \cos\left(
    \cfrac{2\pi\left(x_i-x_{max}\right)}{\left(x_{max}-x_{min}\right)} \right) + ...  + \alpha_5 \cos\left(
    \cfrac{10\pi\left(x_i-x_{max}\right)}{\left(x_{max}-x_{min}\right)} \right)  \right|^2_{\text{ .} }
\end{align}
 
 \ 

 The sum in the RHS of (2) can be put into matrix form:
 
 \begin{align}
     \begin{pmatrix} y_1 \\ y_2 \\ \vdots \\ y_N \\[.25 em]  \end{pmatrix} -
      \begin{pmatrix}
        \\[-.75em]
        b_0\left(x^{(1)}\right) & b_1\left(x^{(1)}\right) & ... &  b_5\left(x^{(1)}\right)   \\[.5em]
        b_0\left(x^{(2)}\right) & b_1\left(x^{(2)}\right) & ...  & b_5\left(x^{(2)}\right)  \\[.5em]
        \vdots & \vdots & \ddots & \vdots  \\[.5em]
        b_0\left(x^{(N)}\right) & b_1\left(x^{(N)}\right) &...  &  b_5\left(x^{(N)}\right)   \\[.5em]
        \end{pmatrix}
        \begin{pmatrix} \alpha_1 \\ \alpha_2 \\ \vdots \\ \alpha_N \\[.25 em]  \end{pmatrix}
 \end{align}

\ 

and (2) is equivalent to 


\begin{align}
    \min\limits_{\alpha \in \mathbb{R}^6 } ||{\bm y}-B{\bm \alpha}||^2. 
\end{align}
 

So given some data we are trying to fit through that data the best $5^{th}$ harmonic. We are not doing regression in the sense of fitting a line because we are fitting a $5^{th}$ harmonic. But we end up doing the same exact thing that we do for linear regression (minimize sum of squares, replace with model matrix).


\ 




The important part of linear regression is not that the functions are linear, it is that the space of the functions we are considering are a linear function space. We are still linear in the parameters. Thus in this context, determining $f(x)$ corresponds to a linear regression.


\ 

Now for projection consider the ${\bm y}$, $B$ and ${\bm \alpha}$ from (3) above. We wish to choose ${\bm \alpha}$ so that $B{\bm \alpha}$ is as close as possible to ${\bm y}$. Or put another way, find the point in the $\text{Span}(B) = \text{Span}\left(b^{(0)} \ \ b^{(1)} \ \ \dots \ \ b^{(5)}\right)$ closest to ${\bm y}$. If $n=2$, we would have a visual like this:

\ 

\hfil \includegraphics{proj2.png}

\ 

For $n=6$, $b_{0}$ through $b_{5}$ are linearly combining to form a plane Again, we want to find the point closest to ${\bm y}$. It will be the point that is orthogonal and this point will be the $\text{Proj}_B(y)$.


\ 

The projection of $x \in \mathbb{R}$ onto $\Omega$ is the closest point in $\Omega$ to $x$.

\begin{align*}
    P_\Omega(x) = \min_{z \in \Omega} ||z-x||
\end{align*}
 
Now suppose $\Omega =  \text{Span}\left(b^{(0)} \ \ b^{(1)} \ \ \dots \ \ b^{(5)}\right)$, and \(\displaystyle P_\Omega(x) = \min_{z \in \Omega} ||z-y|| \), where 

\begin{align*}
     z &= \alpha_0 b^{(0)} + \alpha_1 b^{(1)} + ... + \alpha_5 b^{(5)} = B{\bm \alpha} = 
        \begin{pmatrix}
         b^{(0)} &  b^{(1)}& \dots & b^{(5)}
        \end{pmatrix}
        \begin{pmatrix}
        \alpha_0 & \alpha_1 & \dots & \alpha_5
        \end{pmatrix}^T
\end{align*}
and 
\begin{align} \min_{z \in \Omega} ||z-y|| = \min_{z \in \Omega} ||\alpha_0 b^{(0)} + \alpha_1 b^{(1)} + ... + \alpha_5 b^{(5)}-y|| \end{align}


So we have $\min\limits_{\alpha \in \mathbb{R}} ||B \alpha - y||^2$. To tie projection and linear regression together, we can view linear regression as a projection onto the span of the columns of the model matrix. 

\ 
 

Now tying both of them to quadratic optimization. When we have a linear subspace and project onto it, it leads to a minimization of a quadratic $\min\limits_{\alpha \in \mathbb{R}^6} ||B{\bm \alpha}-{\bm y}||^2$, (5) above. When $\mathcal{F}$ is a linear function space then the regression is a minimization of a quadratic $\min\limits_{\alpha \in \mathbb{R}^6 } ||{\bm y}-B{\bm \alpha}||^2$, (4) above.


\  

Both (4) and (5) are solved with the normal equations. 

\ 


\begin{align*}
L(\alpha) &= \sum_{i=1}^N \left(   y_i - \alpha_0 + \alpha_1x_1^{(i)} + \alpha_2x_2^{(i)} + ... \alpha_nx_n^{(i)}  \right)^2 \\
&= \sum_{i=1}^N r_i^2 \\
&= {\bf r}\cdot{\bf r} \\
&=\left(y-B \alpha \right) \cdot \left(y-B \alpha\right) \\
&=\left(y-B \alpha\right)^T \left(y-B \alpha\right) \\
&= \left(y^T- (B \alpha)^T \right) \left(y-B \alpha\right) \\
&=y^Ty - (B\alpha)^Ty-y^TB\alpha + (B \alpha)^T B \alpha \\
&= y^Ty - 2(B^T y )^T  \alpha +\alpha^T B^T B \alpha \\
\\
\nabla L(\alpha) &= -\cfrac{1}{2} \left(B^T B \right)^{-1} \left(-2B^T y \right) \\
&= \left( B^T B \right)^{-1} B^T y
\end{align*}


We have shown that determining $f(x)$ corresponds to a linear regression, projection of a vector onto a linear space, and a quadratic optimization.



\newpage

```{r, echo=F}
mat<-as.matrix(read.csv("fapprox.csv"))[,-1]
omega<-as.matrix(read.csv("fapprox.csv"))[,-1]
Fx<-function(w,x){   
	#need this matrix for the normalizing Coefficients
	ncoef<-(1/sqrt( diag(mat) )) 
	if(w == 0 ) {return(   rep(1,length(x) ))  } else {
	return( ncoef[w+1]* cos( (2*pi*w*(x-9.4 ) ) /(25.55-9.4 )) )
	} }
```

```{r,eval=F}
Fx<-function(w,x){   
	#need this matrix for the normalizing Coefficients
	ncoef<-(1/sqrt( diag(Fapprox(1001,n=10000, bones$age)) )) 
	if(w == 0 ) {return(   rep(1,length(x) ))  } else {
	return( ncoef[w+1]* cos( (2*pi*w*(x-9.4 ) ) /(25.55-9.4 )) )
		} }
dmat<-function(m,n=10000,x){

MATT<-matrix(0:(m-1),m,1)

dFx<-function(v,x){ if(v != 0) {
	return( -((4*pi^2*v^2)/(16.15^2))* cos( (2*pi*v*(x-9.4 ))/16.15)
	)	} else  {return(rep(1,length(x) ))} }

a=min(x);b=max(x)
h=(b-a)/n;i=0:(n-1)
W<-apply(MATT,1,function(w) dFx(w,a+(i+1)*h) )  
}

Bpp<-dmat(1000+1,n=10000, bones$age)
z<-seq(min(bones$age),max(bones$age),   length.out = 10000 )
h<-z[2]-z[1]


omega<-h*(t(Bpp) %*% Bpp )

B<- model_matrix(bones$age,1000)
n=1001
y<-as.matrix( bones[,4] )
A<-matrix(NA,(n ),3)
A[,1]<-solve( t(B)%*%B + (.0001 * omega) ) %*% t(B)%*% y
A[,2]<-solve( t(B)%*%B + (1 * omega) ) %*% t(B)%*% y
A[,3]<-solve( t(B)%*%B + (100 * omega) ) %*% t(B)%*% y


par(mar=c(4.1,4.1,1,1))
datt<-data.frame(bones$age, B%*% A[,1], B%*% A[,2], B%*% A[,3] )
datt<-datt[with(datt, order(bones$age)), ]

plot(bones[,2], bones[,4], xlab="age", ylab="spnbmd")
lines( datt[,1],datt[,2],col="red", lwd=2)
lines( datt[,1],datt[,3],col="blue", lwd=2)
lines( datt[,1],datt[,4],col="green", lwd=2)
```

\includegraphics{penal.png}

As $\rho$ increases the regression becomes more linear in order to reduce the penalty term, the result is that the fitting term increases, meaning that the fit becomes poorer.


\newpage

 The neural net is a nonconvex optimization, we may go to a local minimum. We want the hessian of the negative log likelihood to be positive definite, or alternatively we want all the eigenvalues to be positive. However, for neural nets the hessian of negative log likelihood is not going to be positive definite because it is not a convex function. 
 
 \ 
 
 However we can modify the hessian such that it is positive definite and similar to the hessian. We can rewrite the hessian as a decomposition, and add a constant so the minimum eigenvalue becomes positive
 
 \begin{align*}
     H&= Q \begin{pmatrix}
     \lambda_1  & &  \\
     & \lambda_2 &  \\
     & & \ddots &  \\
       &  &  &\lambda_n
     \end{pmatrix} Q^T \\
     \\
     H + \lambda I &= Q \begin{pmatrix}
     \lambda_1 + \lambda  & &  \\
     & \lambda_2 + \lambda&  \\
     & & \ddots &  \\
       &  &  &\lambda_n +  \lambda
     \end{pmatrix} Q^T \\
     \\
     &= Q \begin{pmatrix}
     \lambda_1  & &  \\
     & \lambda_2 &  \\
     & & \ddots &  \\
       &  &  &\lambda_n
     \end{pmatrix} Q^T+
     Q \begin{pmatrix}
     \lambda  & &  \\
     & \lambda &  \\
     & & \ddots &  \\
       &  &  &\lambda
     \end{pmatrix} Q^T.
 \end{align*}
 
We should raise $\lambda$ if we are doing poorly, because raising the lambdas enough will get all of our eigenvalues positive. When the eigenvalues are all positive we are in a region of the function that is locally convex and we will go down.

\ 

\ 

First, let's write down $Hf(\eta)d$ 

 \begin{align}
     Hf(\eta)d &= \begin{pmatrix} \\[-.85em]
     \cfrac{ \ \partial^2 f(\eta) \ }{ \partial \eta_1^2} & \cfrac{ \ \partial^2 f(\eta) \ }{ \ \partial \eta_2 \partial \eta_1 \ } & ... & \cfrac{ \ \partial^2 f(\eta) \ }{ \ \partial \eta_n \partial \eta_1 \ } \\[1.25em]
     \cfrac{ \ \partial^2 f(\eta) \ }{ \ \partial \eta_2 \partial \eta_1 \ } & \cfrac{ \ \partial^2 f(\eta) \ }{ \ \partial \eta_2^2   \ } & ... & \cfrac{ \ \partial^2 f(\eta) \ }{ \ \partial \eta_n \partial \eta_2 \ } \\[1em]
     \vdots & \vdots & \ddots & \vdots \\[1em]
    \cfrac{ \ \partial^2 f(\eta) \ }{ \ \partial \eta_n \partial \eta_1 \ } & \cfrac{ \ \partial^2 f(\eta) \ }{ \ \partial \eta_n \partial \eta_2   \ } & ... & \cfrac{ \ \partial^2 f(\eta) \ }{ \ \partial \eta^2_n \ } \\[1.25em]
     \end{pmatrix} \begin{pmatrix} \\[-.85em] d_1     \\[1.25em] d_2  \\[1em] \vdots  \\[1em] d_n  \\[.85em]  \end{pmatrix} \\
     &= \begin{pmatrix} \\[-.85em]
     \cfrac{ \ \partial^2 f(\eta) \ }{ \partial \eta_1^2} \ d_1 + \cfrac{ \ \partial^2 f(\eta) \ }{ \ \partial \eta_2 \partial \eta_1 \ } \ d_2 + ... + \cfrac{ \ \partial^2 f(\eta) \ }{ \ \partial \eta_n \partial \eta_1 \ } \ d_n \\[1.25em]
     \cfrac{ \ \partial^2 f(\eta) \ }{ \ \partial \eta_2 \partial \eta_1 \ } \ d_1 + \cfrac{ \ \partial^2 f(\eta) \ }{ \ \partial \eta_2^2   \ } \ d_2  + ... + \cfrac{ \ \partial^2 f(\eta) \ }{ \ \partial \eta_n \partial \eta_2 \ } \ d_n \\[1em]
    \vdots \\[1em]
    \cfrac{ \ \partial^2 f(\eta) \ }{ \ \partial \eta_n \partial \eta_1 \ } \ d_1 + \cfrac{ \ \partial^2 f(\eta) \ }{ \ \partial \eta_n \partial \eta_2   \ } \ d_2 + ... + \cfrac{ \ \partial^2 f(\eta) \ }{ \ \partial \eta^2_n \ } \ d_n \\[1.25em]
     \end{pmatrix} \\
     &= \begin{pmatrix} 
    \\[-.85em]
   \nabla   \left( \cfrac{\partial }{\partial \eta_1 }  f(\eta) \right) \cdot {\bm d} \\[1.25em]
    \nabla   \left( \cfrac{\partial }{\partial \eta_2 }  f(\eta) \right) \cdot {\bm d} \\[1.25em]
    \vdots \\[1.25em]
    \nabla   \left( \cfrac{\partial }{\partial \eta_i }  f(\eta) \right) \cdot {\bm d} \\[1.25em]
    \vdots \\[1.25em]
    \nabla   \left( \cfrac{\partial }{\partial \eta_n }  f(\eta) \right) \cdot {\bm d} \\[1.25em]
    \end{pmatrix} 
 \end{align}


\ 

Now we can show that

\begin{align*}
     Hf(\eta)d &= \lim_{\epsilon \rightarrow 0} \cfrac{\nabla f(\eta + \epsilon d) - \nabla f(\eta)}{\epsilon} 
 \end{align*}
 
is equal to $Hf(\eta)d$.  Consider the $i$th coordinate of the vector $Hf(\eta)d$.
 
   \begin{align}
     \Big[Hf(\eta)d\Big]_i &= 
     \lim_{\epsilon \rightarrow 0} \cfrac{ \ \cfrac{\partial }{\partial \eta_i }  f(\eta + \epsilon {\bm d}) -   \cfrac{\partial }{\partial \eta_i \ }   f(\eta)}{\epsilon}   
     \end{align}
      
\

First-order Taylor series expansion about the base point $\eta$:
 
 \begin{align*}
     \cfrac{\partial }{\partial \eta_i }  f(\eta + \epsilon {\bm d}) &\approx \cfrac{\partial }{\partial \eta_i }  f(\eta) + \nabla 
     \left( \cfrac{\partial }{\partial \eta_i }  f(\eta) \right) \cdot \epsilon {\bm d}. 
 \end{align*}
 
 Plug back the expansion back into (9)
 
  \begin{align*}
     \Big[Hf(\eta)d\Big]_i &= \lim_{\epsilon \rightarrow 0} \cfrac{ \ \cfrac{\partial }{\partial \eta_i }  f(\eta + \epsilon {\bm d}) -   \cfrac{\partial }{\partial \eta_i \ }   f(\eta)}{\epsilon}   \\
     \\
   & \approx \lim_{\epsilon \rightarrow 0} \cfrac{ \ \cancel{ \cfrac{ \partial }{\partial \eta_i }  f(\eta) } + \nabla 
     \left( \cfrac{\partial }{\partial \eta_i }  f(\eta) \right) \cdot \cancel{ \epsilon} {\bm d} - \cancel{ \cfrac{\partial }{\partial \eta_i \ }   f(\eta)} \ }{ \cancel{\epsilon }}   \\
     &= \nabla   \left( \cfrac{\partial }{\partial \eta_i }  f(\eta) \right) \cdot {\bm d}
\end{align*}

We see that for the $i$th coordinate is equal to the $i$th row in (8) above. Other coordinates will follow by the same argument.




